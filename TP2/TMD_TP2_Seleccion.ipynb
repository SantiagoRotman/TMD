{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1HZX8jkGNAw"
      },
      "source": [
        "# Código TP2: Selección de Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "umcxr9Q6GL1f"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, List\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORxWUkSbmckf"
      },
      "source": [
        "## Función general de uso de distintos métodos para forward ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JYXeKbIIGgFI"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------\n",
        "# AVISO: codigo de demostracion\n",
        "# No es optimo, no es la mejor solucion\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def forward_ranking(\n",
        "        X: np.ndarray, y: np.ndarray,\n",
        "        method: Callable[[np.ndarray, np.ndarray, Dict[str, Any]], float],\n",
        "        verbosity: int = 0, **kwargs) -> List[int]:\n",
        "    \"\"\"\n",
        "    Método Wrapper Forward Selection/Ranking.\n",
        "    Comienza con una lista vacía e iterativamente añade la variable que junto\n",
        "    a las ya seleccionadas retorna el menor error según el método provisto\n",
        "    como argumento.\n",
        "\n",
        "    Argumentos:\n",
        "        X: lista de N inputs de dimensión M como array numpy de shape (N, M).\n",
        "        y: lista de N targets como array numpy de shape (N,).\n",
        "        method: función que estima el classification error para un clasificador\n",
        "            dado con los datos dados; debe poder tomar una lista de inputs y\n",
        "            una lista de targets y los argumentos pasados por \"**kwargs\", y\n",
        "            devolver el error del modelo en el intervalo ( 0 , 1 ).\n",
        "        verbosity: nivel de verbosidad de 0 a 3 que controla cuanta información\n",
        "            se imprime por pantalla; a mayor número, más información imprime.\n",
        "        **kwargs: parámetros adicionales a pasarle a la función de estimación.\n",
        "\n",
        "    Retorna:\n",
        "        orden de importancia de las variables (descendiente, numeradas desde cero)\n",
        "    \"\"\"\n",
        "    _, max_feat = X.shape  # total de features\n",
        "    num_feat = 0           # numero actual de features\n",
        "    # lista para guardar los features elegidos, inicializo como llegaron\n",
        "    list_feat = np.arange(0, max_feat)\n",
        "\n",
        "    # ranking inicial: elijo la variable con menor error de predicción\n",
        "    class_error = np.zeros(max_feat)\n",
        "    # para cada indice i creo el dataset con la variable sola, entreno el\n",
        "    # modelo, y le mido el error, que lo guardo en class_error[i]\n",
        "    for i in range(max_feat):\n",
        "        X_train = X[:, [i]]  # preservamos estructura de matriz\n",
        "        class_error[i] = method(X_train, y, **kwargs)\n",
        "\n",
        "    if verbosity > 2:\n",
        "        print(f'\\nIndividual Classes Error:\\n{class_error}')\n",
        "\n",
        "    # guardo la variable con mínimo error como primera en mi lista de elegidas\n",
        "    # guardo una lista keep_feat con las que me quedan para seguir eligiendo\n",
        "    list_feat[0] = np.argmin(class_error)\n",
        "    keep_feat = np.argsort(class_error)[1:]\n",
        "    num_feat += 1\n",
        "\n",
        "    if verbosity > 1:\n",
        "        print(f'\\nFirst feature: {list_feat[0]}')\n",
        "\n",
        "    # loop principal\n",
        "    # a cada paso agrego todas las variables disponibles, de a una, les mido el\n",
        "    # error y me quedo con la de mínimo error hasta llegar a meter todas\n",
        "    while num_feat < max_feat:\n",
        "        class_error = np.zeros(max_feat - num_feat)\n",
        "\n",
        "        # class_error guarda el error de cada modelo restante\n",
        "        for i in range(max_feat - num_feat):\n",
        "            features = np.concatenate((list_feat[0:num_feat], [keep_feat[i]]))\n",
        "            X_train = X[:, features]\n",
        "            class_error[i] = method(X_train, y, **kwargs)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nFeatures:\\n{keep_feat}\\nErrors:\\n{class_error}')\n",
        "\n",
        "        # me quedo con el modelo de minimo error, guardo ese feature en la lista\n",
        "        # de las elegidas, lo saco de la lista de las que me quedan\n",
        "        best_index = np.argmin(class_error)\n",
        "        list_feat[num_feat] = keep_feat[best_index]\n",
        "\n",
        "        if verbosity > 1:\n",
        "            print(f'\\n------------\\nStep {num_feat+1}\\nFeature {best_index}')\n",
        "\n",
        "        keep_feat = np.delete(keep_feat, best_index)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nNew search list {keep_feat}')\n",
        "\n",
        "        num_feat += 1\n",
        "\n",
        "    if verbosity > 1:\n",
        "        print(f'\\n------------\\nFinal ranking of features: {list_feat}\\n')\n",
        "\n",
        "    return list_feat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po8uwSFfmoQG"
      },
      "source": [
        "## Métodos para entrenamiento y estimación de error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u5DrtGQ0lyNC"
      },
      "outputs": [],
      "source": [
        "def error_rate(A, B):\n",
        "    \"\"\"  Función de ayuda para calcular error \"\"\"\n",
        "    return sum(A != B) / len(B)\n",
        "\n",
        "\n",
        "def RF_error_estimation(\n",
        "        X, y, equalize_classes = True, total_trees = 500, mtry = 0):\n",
        "    \"\"\"\n",
        "    Random forest error estimation (Out-Of-Bag) for greedy search\n",
        "    \"\"\"\n",
        "    if mtry < 1:\n",
        "        mtry = int(np.floor(np.sqrt(X.shape[1])))\n",
        "    class_weights = None\n",
        "    if equalize_classes:\n",
        "        class_weights = 'balanced'\n",
        "    random_forest = RandomForestClassifier(\n",
        "        n_estimators=total_trees, max_features=mtry,\n",
        "        bootstrap=True, class_weight=class_weights, oob_score=True)\n",
        "    random_forest.fit(X, y)\n",
        "    return 1 - random_forest.oob_score_  # final out-of-bag error\n",
        "\n",
        "\n",
        "def LDA_error_estimation(X, y):\n",
        "    \"\"\"\n",
        "    LDA error estimation (Leave-One-Out) for greedy search\n",
        "    \"\"\"\n",
        "    loo = LeaveOneOut()\n",
        "    y_pred = np.empty_like(y)  # empty results array\n",
        "    for train_index, test_index in loo.split(X):\n",
        "        lda = LinearDiscriminantAnalysis()\n",
        "        lda.fit(X[train_index], y[train_index])\n",
        "        y_pred[test_index] = lda.predict(X[test_index])\n",
        "    return error_rate(y, y_pred)\n",
        "\n",
        "\n",
        "def SVM_error_estimation(\n",
        "        X, y, C = 1.0, cross = 4):\n",
        "    \"\"\"\n",
        "    SVM error estimation (internal Cross-Validation) for greedy search\n",
        "    \"\"\"\n",
        "    svm = LinearSVC(C=C)\n",
        "    return 1 - cross_val_score(svm, X, y, cv=cross).mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RpGL1SC7Su"
      },
      "source": [
        "## Métodos de Ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gfZmRe6RC7Sv"
      },
      "outputs": [],
      "source": [
        "def RF_ranking_method(\n",
        "        X, y, equalize_classes = True, total_trees = 500, mtry = 0):\n",
        "    \"\"\"\n",
        "    Random forest ranking method for rfe\n",
        "    \"\"\"\n",
        "    if mtry < 1:\n",
        "        mtry = int(np.floor(np.sqrt(X.shape[1])))\n",
        "\n",
        "    class_weights = None\n",
        "    if equalize_classes:\n",
        "        class_weights = 'balanced'\n",
        "\n",
        "    random_forest = RandomForestClassifier(\n",
        "        n_estimators=total_trees, max_features=mtry,\n",
        "        bootstrap=True, class_weight=class_weights, oob_score=True)\n",
        "    random_forest.fit(X, y)\n",
        "\n",
        "    importance_values = random_forest.feature_importances_\n",
        "    feats = np.argsort(importance_values)\n",
        "    imp = importance_values[feats]\n",
        "\n",
        "    return (feats, imp)\n",
        "\n",
        "\n",
        "def LSVM_ranking_method(X, y, C = 100):\n",
        "    \"\"\"\n",
        "    Linear svm ranking method for rfe. Multiclass\n",
        "    \"\"\"\n",
        "    svm = LinearSVC(C=C)\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    weights = np.zeros(len(np.unique(y)))\n",
        "    # coef_ is of shape (1, nfeat) if 2-class, (nclass, nfeat) otherwise\n",
        "    coefs = np.abs(svm.coef_)\n",
        "    # Sumar la importancia de cada característica a través de los modelos\n",
        "    w = np.sum(coefs, axis=0)\n",
        "\n",
        "    feats = np.argsort(w)\n",
        "    imp = w[feats]\n",
        "    return (feats, imp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxlN7cLzHSr-"
      },
      "source": [
        "## Ejemplos de Uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bTC2SyVmrDF",
        "outputId": "1f1f11e8-be70-4634-ccf8-3aa364d7523f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 150 (50 in each of three classes)\n",
            ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
            ":Attribute Information:\n",
            "    - sepal length in cm\n",
            "    - sepal width in cm\n",
            "    - petal length in cm\n",
            "    - petal width in cm\n",
            "    - class:\n",
            "            - Iris-Setosa\n",
            "            - Iris-Versicolour\n",
            "            - Iris-Virginica\n",
            "\n",
            ":Summary Statistics:\n",
            "\n",
            "============== ==== ==== ======= ===== ====================\n",
            "                Min  Max   Mean    SD   Class Correlation\n",
            "============== ==== ==== ======= ===== ====================\n",
            "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
            "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
            "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
            "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
            "============== ==== ==== ======= ===== ====================\n",
            "\n",
            ":Missing Attribute Values: None\n",
            ":Class Distribution: 33.3% for each of 3 classes.\n",
            ":Creator: R.A. Fisher\n",
            ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            ":Date: July, 1988\n",
            "\n",
            "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
            "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
            "Machine Learning Repository, which has two wrong data points.\n",
            "\n",
            "This is perhaps the best known database to be found in the\n",
            "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
            "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
            "data set contains 3 classes of 50 instances each, where each class refers to a\n",
            "type of iris plant.  One class is linearly separable from the other 2; the\n",
            "latter are NOT linearly separable from each other.\n",
            "\n",
            "|details-start|\n",
            "**References**\n",
            "|details-split|\n",
            "\n",
            "- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
            "  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
            "  Mathematical Statistics\" (John Wiley, NY, 1950).\n",
            "- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
            "  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
            "- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
            "  Structure and Classification Rule for Recognition in Partially Exposed\n",
            "  Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
            "  Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
            "- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
            "  on Information Theory, May 1972, 431-433.\n",
            "- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
            "  conceptual clustering system finds 3 classes in the data.\n",
            "- Many, many more ...\n",
            "\n",
            "|details-end|\n",
            "\n",
            "Iris Inputs (X) shape: (150, 4)\n",
            "Iris Targets (y) shape: (150,)\n"
          ]
        }
      ],
      "source": [
        "# Includes de distintos métodos\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "# Exploramos dataset iris\n",
        "iris = datasets.load_iris()\n",
        "print(iris.DESCR)\n",
        "\n",
        "print(\"Iris Inputs (X) shape:\", iris.data.shape)\n",
        "print(\"Iris Targets (y) shape:\", iris.target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frpAF7NzHUaf",
        "outputId": "38c72229-fd46-4dc1-dbc0-67ce554482bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: [3 2 0 1]\n",
            "RF2: [3 2 1 0]\n",
            "LDA: [3 2 0 1]\n",
            "SVM: [3 1 2 0]\n"
          ]
        }
      ],
      "source": [
        "# Probamos el metodo forward-ranking con IRIS\n",
        "iris_forward_rf = forward_ranking(\n",
        "    iris.data, iris.target,\n",
        "    method=RF_error_estimation, total_trees=100,\n",
        "    equalize_classes=False, verbosity=0)\n",
        "print(f'RF: {iris_forward_rf}')\n",
        "\n",
        "iris_forward_rf2 = forward_ranking(\n",
        "    iris.data, iris.target,\n",
        "    method=RF_error_estimation, total_trees=100,\n",
        "    equalize_classes=True, verbosity=0)\n",
        "print(f'RF2: {iris_forward_rf2}')\n",
        "\n",
        "iris_forward_lda = forward_ranking(\n",
        "    iris.data, iris.target, method=LDA_error_estimation)\n",
        "print(f'LDA: {iris_forward_lda}')\n",
        "\n",
        "iris_forward_svm = forward_ranking(\n",
        "    iris.data, iris.target, method=SVM_error_estimation)\n",
        "print(f'SVM: {iris_forward_svm}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XOlTKD5TC7Sx"
      },
      "outputs": [],
      "source": [
        "# Creador de ruido uniforme multimodal\n",
        "def crea_ruido_unif(n = 100, d = 2):\n",
        "    X = np.random.uniform(-1, 1, size=(2*n,d))\n",
        "    y = np.repeat([1,-1], [n,n])\n",
        "    return (X,y)  # NOTA: difiere de implem. R\n",
        "\n",
        "\n",
        "# Crea dataset datosA\n",
        "# dataset artificial con orden de importancia: 7-5-3-1\n",
        "d = 10\n",
        "n = 1000\n",
        "datosAX, datosAy = crea_ruido_unif(n=n, d=d)\n",
        "# tomar 50% de los datos al azar, y hacer que la clase sea el signo\n",
        "# de la 8 variable\n",
        "shuffle = np.random.permutation(len(datosAy))\n",
        "sub = shuffle[:int(len(datosAy) * 0.5)]\n",
        "datosAy[sub] = np.sign(datosAX[sub, 7])\n",
        "# tomar 20% de los datos al azar (fuera de los anteriores), y hacer\n",
        "# que la clase sea el signo de la 6 variable\n",
        "sub = shuffle[int(len(datosAy) * 0.5):int(len(datosAy) * 0.7)]\n",
        "datosAy[sub] = np.sign(datosAX[sub, 5])\n",
        "# tomar 10% de los datos al azar, y hacer que la clase sea el signo de\n",
        "# la 4 variable\n",
        "sub = shuffle[int(len(datosAy) * 0.7):int(len(datosAy) * 0.8)]\n",
        "datosAy[sub] = np.sign(datosAX[sub, 3])\n",
        "# tomar 5% de los datos al azar, y hacer que la clase sea el signo de\n",
        "# la 2 variable\n",
        "sub = shuffle[int(len(datosAy) * 0.8):int(len(datosAy) * 0.85)]\n",
        "datosAy[sub] = np.sign(datosAX[sub, 1])\n",
        "\n",
        "\n",
        "# Crea dataset datosB\n",
        "# dataset artificial con dos variables relevantes (0-1) y dos variables\n",
        "# que son importantes pero que no resuelven el problema (2-3)\n",
        "d = 8\n",
        "n = 1000\n",
        "datosBX, datosBy = crea_ruido_unif(n=n, d=d)\n",
        "# hacer que la clase sea el xor de las 2 primeras variables (es usando el signo)\n",
        "datosBy = np.sign(datosBX[:, 0] * datosBX[:, 1])\n",
        "# hacer que las variables 3 y 4 tengan un 50% de correlacion con la clase\n",
        "shuffle = np.random.permutation(len(datosBy))\n",
        "sub = shuffle[:int(len(datosBy) * 0.5)]\n",
        "datosBX[sub, 2] = np.abs(datosBX[sub, 2] * datosBy[sub])\n",
        "shuffle = np.random.permutation(len(datosBy))\n",
        "sub = shuffle[:int(len(datosBy) * 0.5)]\n",
        "datosBX[sub, 3] = np.abs(datosBX[sub, 3] * datosBy[sub])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapper greedy backward "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_ranking(\n",
        "        X: np.ndarray, y: np.ndarray,\n",
        "        method: Callable[[np.ndarray, np.ndarray, Dict[str, Any]], float],\n",
        "        verbosity: int = 0, **kwargs) -> List[int]:\n",
        "    _, max_feat = X.shape  # total de features\n",
        "    num_feat = 0           # numero actual de features\n",
        "    keep_feat = np.arange(0, max_feat)  # lista de los que me quedan para seguir eligiendo\n",
        "    del_feat = np.arange(0, max_feat)  # lista para guardar los features eliminados\n",
        "\n",
        "    # loop principal\n",
        "    # a cada paso agrego todas las variables disponibles, de a una, les mido el\n",
        "    # error y me quedo con la de mínimo error hasta llegar a meter todas\n",
        "    while num_feat < max_feat-1:\n",
        "        class_error = np.zeros(max_feat - num_feat)\n",
        "\n",
        "        # class_error guarda el error de cada modelo restante\n",
        "        for i in range(max_feat - num_feat):\n",
        "            features = np.delete(keep_feat, i)\n",
        "            X_train = X[:, features]\n",
        "            class_error[i] = method(X_train, y, **kwargs)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nFeatures:\\n{keep_feat}\\nErrors:\\n{class_error}')\n",
        "\n",
        "        # saco la variable del modelo con menos error (porque es la menos importante)\n",
        "        worst_index = np.argmin(class_error)\n",
        "        del_feat[num_feat] = keep_feat[worst_index]\n",
        "\n",
        "        if verbosity > 1:\n",
        "            print(f'\\n------------\\nStep {num_feat+1}\\nFeature {worst_index}')\n",
        "\n",
        "        keep_feat = np.delete(keep_feat, worst_index)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nNew search list {keep_feat}')\n",
        "\n",
        "        num_feat += 1\n",
        "\n",
        "    del_feat[num_feat] = keep_feat[0]\n",
        "    \n",
        "    if verbosity > 1:\n",
        "        print(f'\\n------------\\nFinal ranking of features: {np.flip(del_feat)}\\n')\n",
        "\n",
        "    return np.flip(del_feat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RF: [3 2 0 1]\n",
            "RF2: [3 2 1 0]\n",
            "LDA: [3 2 0 1]\n",
            "SVM: [2 1 0 3]\n"
          ]
        }
      ],
      "source": [
        "# Probamos el metodo backward-ranking con IRIS\n",
        "iris_backward_rf = backward_ranking(\n",
        "    iris.data, iris.target,\n",
        "    method=RF_error_estimation, total_trees=100,\n",
        "    equalize_classes=False, verbosity=0)\n",
        "print(f'RF: {iris_backward_rf}')\n",
        "\n",
        "iris_backward_rf2 = backward_ranking(\n",
        "    iris.data, iris.target,\n",
        "    method=RF_error_estimation, total_trees=100,\n",
        "    equalize_classes=True, verbosity=0)\n",
        "print(f'RF2: {iris_backward_rf2}')\n",
        "\n",
        "iris_backward_lda = backward_ranking(\n",
        "    iris.data, iris.target, method=LDA_error_estimation)\n",
        "print(f'LDA: {iris_backward_lda}')\n",
        "\n",
        "iris_backward_svm = backward_ranking(\n",
        "    iris.data, iris.target, method=SVM_error_estimation)\n",
        "print(f'SVM: {iris_backward_svm}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter con test no-paramétrico (Kruskal-Wallis) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "def filter_kruskal(\n",
        "        X: np.ndarray, y: np.ndarray,\n",
        "        verbosity: int = 0, **kwargs) -> List[int]:\n",
        "    _, max_feat = X.shape  # total de features\n",
        "\n",
        "    stats = np.arange(0, max_feat)\n",
        "    #list_feat = np.arange(0, max_feat)\n",
        "    for num_feat in range(max_feat):\n",
        "        x = X[:, num_feat]\n",
        "        stats[num_feat] = kruskal(x,y).statistic\n",
        "\n",
        "    if verbosity > 2:\n",
        "        print(f'\\n------------\\nStats of features: {stats}\\n')\n",
        "    order = np.argsort(stats)\n",
        "    #list_feat = list_feat[order]\n",
        "    if verbosity > 1:\n",
        "        print(f'\\n------------\\nFinal ranking of features: {order}\\n')\n",
        "        \n",
        "    return order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kruskal: [3 2 1 0]\n"
          ]
        }
      ],
      "source": [
        "iris_kruskal = filter_kruskal(\n",
        "    iris.data, iris.target,\n",
        "    verbosity=0)\n",
        "print(f'Kruskal: {iris_kruskal}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eliminación recursiva de características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rfe_ranking(\n",
        "        X: np.ndarray, y: np.ndarray,\n",
        "        method: Callable[[np.ndarray, np.ndarray, Dict[str, Any]], float],\n",
        "        verbosity: int = 0, **kwargs) -> List[int]:\n",
        "    _, max_feat = X.shape  # total de features\n",
        "    num_feat = 0           # numero actual de features\n",
        "    keep_feat = np.arange(0, max_feat)  # lista de los que me quedan para seguir eligiendo\n",
        "    del_feat = np.arange(0, max_feat)  # lista para guardar los features eliminados\n",
        "\n",
        "    # loop principal\n",
        "    # a cada paso agrego todas las variables disponibles, de a una, les mido el\n",
        "    # error y me quedo con la de mínimo error hasta llegar a meter todas\n",
        "    while num_feat < max_feat-1:\n",
        "        class_error = np.zeros(max_feat - num_feat)\n",
        "\n",
        "        # class_error guarda el error de cada modelo restante\n",
        "        for i in range(max_feat - num_feat):\n",
        "            features = np.delete(keep_feat, i)\n",
        "            X_train = X[:, features]\n",
        "            class_error[i] = method(X_train, y, **kwargs)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nFeatures:\\n{keep_feat}\\nErrors:\\n{class_error}')\n",
        "\n",
        "        # saco la variable del modelo con menos error (porque es la menos importante)\n",
        "        worst_index = np.argmin(class_error)\n",
        "        del_feat[num_feat] = keep_feat[worst_index]\n",
        "\n",
        "        if verbosity > 1:\n",
        "            print(f'\\n------------\\nStep {num_feat+1}\\nFeature {worst_index}')\n",
        "\n",
        "        keep_feat = np.delete(keep_feat, worst_index)\n",
        "\n",
        "        if verbosity > 2:\n",
        "            print(f'\\nNew search list {keep_feat}')\n",
        "\n",
        "        num_feat += 1\n",
        "\n",
        "    del_feat[num_feat] = keep_feat[0]\n",
        "    \n",
        "    if verbosity > 1:\n",
        "        print(f'\\n------------\\nFinal ranking of features: {np.flip(del_feat)}\\n')\n",
        "\n",
        "    return np.flip(del_feat)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
